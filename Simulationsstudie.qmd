---
title: "Simulationsstudie"
format: html
editor: visual
---

```{r}
#| echo: false
pacman::p_load("VIM","rgl","MASS","tidyverse","mice")
```

## Code von Julius Simulationsstudie wie bei Julius

Wir brauchen mindestens eine Variable mehr als die transformierte X\^2 um MAR erzeugen zu können.

Da der Text sich zu erst mit X\^2 beschäftigt, glaube ich dass es sinnvoll ist das ebenfalls zu tun

X3 ist die transformierte Variable X2\^2

Notwendigkeit von Widerholung damit Simulationsstudie

Daten generieren und Missings
```{r}
set.seed(1)
  
n <- 10000
X1 <- rnorm(n, 8, 3) # Prädiktor für X2,X3, Y
X2 <- rnorm(n, 0, 3) # weitere Variable damit MAR erzeugt werden kann
X3 <- X2^2 # transformierte Variable
Y <- 5 + 0.6 * X1 + 0.5 * X2 + 0.3*X3+ rnorm(n, 0, sqrt(2)) # Outcome Variable
  
data1 <- as.data.frame(cbind(X1, X2, X3, Y))
  
  # Quantities of interest for the original data set
(bef.Imp2 <- cbind(mean = mean(X2), # Überlegen ob ich Mean brauche oder den True Mean den ich in die Daten gebe
                    var = var(X2)))
  
(bef.Imp3 <- cbind(mean = mean(X3),
                     var = var(X3)))
  
X2NonNA <- X2 # save for later
X3NonNA <- X3 # save for later
  
# Generate missing values (MCAR)
data1$X2[sample(x = 1:n, size = n/2, replace = FALSE)] <- NA
data1$X3[sample(x = 1:n, size = n/2, replace = FALSE)] <- NA
  
# Missing indicators for missing data in X3
misind2 <- is.na(data1$X2)
misind3 <- is.na(data1$X3)
# Indicators for observed (not missing) data in X3
obsind2 <- !is.na(data1$X2)
obsind3 <- !is.na(data1$X3)
  
  
#layout(matrix(c(1,2,2), nrow=1))
barMiss(data1,pos = 4) 
# pos- Argument spezifiziert welche Variable betrachtet wird
marginplot(data1[, c("X2", "Y")])
marginplot(data1[, c("X3", "Y")])

# Pattern
md.pattern(data1)
VIM::aggr(data1)
```


# Impute than transform von Julius

```{r}
M <- 5
imp <- mice(data1, print = FALSE, seed = 7112, m = M)
plot(imp) # Konvergenz sieht doch ganz gut aus
com_data1 <- complete(imp, "long", include = TRUE)
com_data1$X3 <- with(com_data1 , X2^2)

summary(com_data1$.imp)

# Wir brauchen nur transformierte Daten
imp_data1 <- com_data1[com_data1$.imp == M, ] # eins ist falsch

# Bias untersuchen
(after.Imp2 <- cbind(mean = mean(imp_data1$X2),
                    var = var(imp_data1$X2)))

(after.Imp3 <- cbind(mean = mean(imp_data1$X3),
                    var = var(imp_data1$X3)))
bias_2 <- after.Imp2[,"mean"]-bef.Imp2[,"mean"]
bias_3 <- after.Imp3[,"mean"]-bef.Imp3[,"mean"]

# MSE
mse_2 <- (after.Imp2[,"mean"]-bef.Imp2[,"mean"])^2
mse_3 <- (after.Imp3[,"mean"]-bef.Imp3[,"mean"])^2

#imp.itt <- as.mids(imp_data1)   # Converts an imputed dataset (long format) into a mids object


  
# -> transformed variable cannot be used for imputation..

#

#plot the imputed values
long_Imp1 <- 
  imp_data1  %>% # gefiltert weil imp == 1 imputierte daten sind
  mutate("X2" = imp$where[,2],
         "X3" = imp$where[,3])# %>% 
 # mutate("X3_imp" = imp$whereX)
marginplot(x = long_Imp1[,c("X2","X3")], delimiter = "_imp")

marginplot(x = long_Imp1[,c("X2","X3","X3_imp")], delimiter = "_imp")




```

# Transform than Impute von Julius

```{r}
data1$X3 <- data1$X2^2

imp.jav1 <- mice(data, seed = 32093, print = FALSE)
pred <- make.predictorMatrix(data)
pred

# set relevant entries to 0 to prevent automatic removal
pred[c("wgt", "whr"), c("wgt", "whr")] <- 0
pred[c("hgt", "whr"), c("hgt", "whr")] <- 0
pred

imp.jav2 <- mice(data, pred = pred, seed = 32093, print = TRUE)
```

# Mit Wiederholungen
```{r}

S <- 10
n <- 100
M <- 5

# Liste der Variablennamen
TrueMeanVec2 <- numeric(S)
TrueMeanVec3 <- numeric(S)
ImpMeanVec2 <- numeric(S)
ImpMeanVec3 <- numeric(S)
BiasVec2 <- numeric(S)
BiasVec3 <- numeric(S)
MSEVec2 <- numeric(S)
MSEVec3 <- numeric(S)
UpperBoundVec2 <- numeric(S)
LowerBoundVec2 <- numeric(S)
UpperBoundVec3 <- numeric(S)
LowerBoundVec3 <- numeric(S)

for (s in 1:S) {
  # Daten erzeugen
  X1 <- rnorm(n, 8, 3) # Prädiktor für X2,X3, Y
  X2 <- 10 - 0.5 * X1 + rnorm(n, 0, 3) # weitere Variable damit MAR erzeugt werden   kann (noch nicht)
  X3 <- X2^2 # transformierte Variable
  Y <- 5 + 0.6 * X1 + 0.5 * X2 + 0.3*X3+ rnorm(n, 0, sqrt(2)) # Outcome Variable
  
    # Quantities of interest for the original data set
  (bef.Imp2 <- cbind(mean = mean(X2),
                    var = var(X2)))
  
  (bef.Imp3 <- cbind(mean = mean(X3),
                     var = var(X3)))
  data1 <- as.data.frame(cbind(X1, X2, X3, Y))
  
  # Generate missing values (MCAR)
  data1$X2[sample(x = 1:n, size = n/2, replace = FALSE)] <- NA
  data1$X3[sample(x = 1:n, size = n/2, replace = FALSE)] <- NA
  # Impute, than transform
  imp <- mice(data1, print = FALSE, seed = 7112)
  com_data1 <- complete(imp, "long", include = TRUE)
  com_data1$X3 <- with(com_data1 , X2^2)
  
  # Wir brauchen nur transformierte Daten
  imp_data1 <- com_data1[com_data1$.imp == M, ]


  # Bias untersuchen
  (after.Imp2 <- cbind(mean = mean(imp_data1$X2),
                      var = var(imp_data1$X2)))
  
  (after.Imp3 <- cbind(mean = mean(imp_data1$X3),
                      var = var(imp_data1$X3)))
  
  TrueMeanVec2[s] <- bef.Imp2[,"mean"]
  TrueMeanVec3[s] <- bef.Imp3[,"mean"]
  ImpMeanVec2[s] <- after.Imp2[,"mean"]
  ImpMeanVec3[s] <- after.Imp3[,"mean"]
  BiasVec2[s] <- after.Imp2[,"mean"]-bef.Imp2[,"mean"]
  BiasVec3[s] <- after.Imp3[,"mean"]-bef.Imp3[,"mean"]
  
  # MSE
  MSEVec2[s] <- (after.Imp2[,"mean"]-bef.Imp2[,"mean"])^2
  MSEVec3[s] <- (after.Imp3[,"mean"]-bef.Imp3[,"mean"])^2
  
  #calculate bounds of interval 
  UpperBoundVec2[s] <- ImpMeanVec2[s] + 1.96 * sqrt(after.Imp2[,"var"]) / sqrt(n) 
  LowerBoundVec2[s] <- ImpMeanVec2[s] - 1.96 * sqrt(after.Imp2[,"var"]) / sqrt(n)
  
  UpperBoundVec3[s] <- ImpMeanVec3[s] + 1.96 * sqrt(after.Imp3[,"var"]) / sqrt(n) 
  LowerBoundVec3[s] <- ImpMeanVec3[s] - 1.96 * sqrt(after.Imp3[,"var"]) / sqrt(n)
  
}

TotDat <- cbind("Id" = 1:S,
                 TrueMeanVec2,ImpMeanVec2,TrueMeanVec3, ImpMeanVec3,BiasVec2, BiasVec3,UpperBoundVec3, LowerBoundVec3,MSEVec2,MSEVec3, UpperBoundVec2,LowerBoundVec2)

colMeans(TotDat)


```

Impute, than transform mit Wiederholungen
- Bias, MSE, Lower and Upperbound von Konfidenzintervallen
- Coverage
- CI- range könnte man noch machen
```{r}
set.seed(123) # Seed für Reproduzierbarkeit
S <- 200 # Anzahl der Simulationen
n <- 50 # Stichprobengröße
M <- 5 # Anzahl an Iteration bei MI

# Vektoren initialisieren
TrueMeanVec2 <- numeric(S)
TrueMeanVec3 <- numeric(S)
ImpMeanVec2 <- numeric(S)
ImpMeanVec3 <- numeric(S)
BiasVec2 <- numeric(S)
BiasVec3 <- numeric(S)
MSEVec2 <- numeric(S)
MSEVec3 <- numeric(S)
UpperBoundVec2 <- numeric(S)
LowerBoundVec2 <- numeric(S)
UpperBoundVec3 <- numeric(S)
LowerBoundVec3 <- numeric(S)

for (s in 1:S) {
  X1 <- rnorm(n, 8, 3)
  X2 <- 10 - 0.5 * X1 + rnorm(n, 0, 3)
  X3 <- X2^2
  Y <- 5 + 0.6 * X1 + 0.5 * X2 + 0.3 * X3 + rnorm(n, 0, sqrt(2))
  
  bef.Imp2 <- cbind(mean = mean(X2), var = var(X2))
  bef.Imp3 <- cbind(mean = mean(X3), var = var(X3))
  
  data1 <- as.data.frame(cbind(X1, X2, X3, Y))
  data1$X2[sample(x = 1:n, size = n/2, replace = FALSE)] <- NA
  data1$X3[sample(x = 1:n, size = n/2, replace = FALSE)] <- NA
  
  imp <- mice(data1, print = FALSE, seed = s)
  com_data1 <- complete(imp, "long", include = TRUE)
  com_data1$X3 <- with(com_data1, X2^2)
  imp_data1 <- com_data1[com_data1$.imp == M, ]
  
  after.Imp2 <- cbind(mean = mean(imp_data1$X2), var = var(imp_data1$X2))
  after.Imp3 <- cbind(mean = mean(imp_data1$X3), var = var(imp_data1$X3))
  
  TrueMeanVec2[s] <- bef.Imp2[,"mean"]
  TrueMeanVec3[s] <- bef.Imp3[,"mean"]
  ImpMeanVec2[s] <- after.Imp2[,"mean"]
  ImpMeanVec3[s] <- after.Imp3[,"mean"]
  BiasVec2[s] <- after.Imp2[,"mean"] - bef.Imp2[,"mean"]
  BiasVec3[s] <- after.Imp3[,"mean"] - bef.Imp3[,"mean"]
  MSEVec2[s] <- (after.Imp2[,"mean"] - bef.Imp2[,"mean"])^2
  MSEVec3[s] <- (after.Imp3[,"mean"] - bef.Imp3[,"mean"])^2
  UpperBoundVec2[s] <- ImpMeanVec2[s] + 1.96 * sqrt(after.Imp2[,"var"]) / sqrt(n)
  LowerBoundVec2[s] <- ImpMeanVec2[s] - 1.96 * sqrt(after.Imp2[,"var"]) / sqrt(n)
  UpperBoundVec3[s] <- ImpMeanVec3[s] + 1.96 * sqrt(after.Imp3[,"var"]) / sqrt(n)
  LowerBoundVec3[s] <- ImpMeanVec3[s] - 1.96 * sqrt(after.Imp3[,"var"]) / sqrt(n)

}



TotDat <- data.frame(
  "Iteration" = 1:S,
  TrueMeanVec2, ImpMeanVec2, TrueMeanVec3, ImpMeanVec3,
  BiasVec2, BiasVec3, UpperBoundVec3, LowerBoundVec3,
  MSEVec2, MSEVec3, UpperBoundVec2, LowerBoundVec2
)
head(TotDat)
class(TotDat)


Conf_Inter_2 <- TotDat  %>% filter( Iteration < 101) %>% 
  ggplot(aes(x = Iteration)) +
  geom_errorbar(aes(ymin = UpperBoundVec2, ymax = LowerBoundVec2)) +
  geom_point(aes(y   = TrueMeanVec2), col = "#593196")+
  theme_bw()

Conf_Inter_3 <- TotDat  %>% filter( Iteration < 101) %>% 
  ggplot(aes(x = Iteration)) +
  geom_errorbar(aes(ymin = UpperBoundVec3, ymax = LowerBoundVec3)) +
  geom_point(aes(y = TrueMeanVec3), col = "#593196")+
  theme_bw()

colMeans(TotDat)

TotDat$CIrange2 <- UpperBoundVec2-LowerBoundVec2
TotDat$CIrange3 <-UpperBoundVec3-LowerBoundVec3
Coverage2 <- TotDat  %>% 
  summarise("Coverage2" = mean((LowerBoundVec2 <= TrueMeanVec2 & UpperBoundVec2 >= TrueMeanVec2)))

# Coverage 2 liegt bei 1, dass finde ich schon krass

Coverage3 <- TotDat %>% 
  summarise("Coverage3" = mean((LowerBoundVec3<= TrueMeanVec3 & UpperBoundVec3 >= TrueMeanVec3)))



```

Regression von Julius
```{r}
# imputation for multiple copies of the data
M <- 3
imp <- mice(SLID, m = M, printFlag = FALSE) 
impDat <- complete(imp, action = "long")

# analysis on all parallel datasets independently
mod <- list() #create empty list
for(i in 1:M){
  mod[[i]] <- data1 %>% filter(.imp == i) %>% 
              lm(Y ~ X1 + X2 + X3,data = .) 
}

# pooling the results

# MI estimator
Theta.hat <- apply(sapply(mod, coef), 1, mean)
```

```{r}
## functions for diagnostics

coverage <- function(true.value, CI.low, CI.up){
  ifelse(test = CI.low <= true.value && CI.up >= true.value, yes = 1, no = 0)
}

rel.bias <- function(true.value, est) {
  rel_bias <- (est - true.value) / true.value
  return(rel_bias)
}
```

## function for Rubin's combining rules, gives pooled estimates, CI
```{r}
MI.analysis <- function(Q.hat,U.hat,m){
  
  if (class(Q.hat)=="matrix") {
    # pooled estimator
    Q.bar <- colSums(Q.hat)/m
    # within-variance
    U.bar <- colSums(U.hat)/m
    # between-variance
    B <- colSums((Q.hat - matrix(1, nrow=m, ncol=1) %*% Q.bar)^2)/(m-1)       
  }
  else{
    Q.bar <- sum(Q.hat)/m
    U.bar <- sum(U.hat)/m
    B <- (1/(m-1))*sum((Q.hat-Q.bar)^2) 
  }
  
  # total variance
  Tot <- U.bar+B+B/m
  # degrees of freedom
  df <- (m-1)*(1+(m/(m+1))*U.bar/B)^2
  # confidence intervals
  CIlow <- Q.bar-qt(0.975,df)*sqrt(Tot)
  CIup <- Q.bar+qt(0.975,df)*sqrt(Tot)
  r <- (B+B/m)/U.bar
  # fraction of missing information
  FMI <- (r+2/(df+3))/(1+r)
  # t-test
  t_value <- Q.bar/sqrt(Tot)
  # p-value
  p.value <-2*(1-pt(abs(t_value),df))
  
  return(cbind(Q.bar,CIlow,CIup))
}

quantVar <- function(y, p = 0.9){
  quant <- quantile(y, prob = p)
  sigma <- (p*(1-p))/(dens_y(quant)^2)
  return(sigma)
}
```
